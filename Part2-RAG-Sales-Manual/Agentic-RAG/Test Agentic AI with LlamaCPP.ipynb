{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fefdb023-6740-4846-810f-9ee9e94615d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MILVUS_HOST=\"milvus-service\"\n",
    "MILVUS_PORT=\"19530\"\n",
    "\n",
    "LLAMA_HOST=\"llama-service\"\n",
    "LLAMA_PORT=\"8080\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ffff728-32e0-46a0-b4e9-2d3c8d90b512",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pymilvus import connections, utility\n",
    "\n",
    "# Connect to Milvus Database\n",
    "connections.connect(host=\"milvus-service\", port=\"19530\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "741ad8cf-ff06-447e-80b6-7a3a6d26c909",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from pymilvus import connections, utility\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.tools import tool\n",
    "from langchain.tools.render import render_text_description_and_args\n",
    "from langchain.agents.output_parsers import JSONAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_log_to_str\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnablePassthrough"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce682a7-98b3-4b96-9797-955f9b56df42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "FNAME = \"granite-3.0-8b-instruct-Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(FNAME):\n",
    "    res = requests.get('https://huggingface.co/bartowski/granite-3.0-8b-instruct-GGUF/resolve/main/granite-3.0-8b-instruct-Q4_K_M.gguf')\n",
    "    with open(FNAME, 'wb') as file:\n",
    "        file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4084ecf2-4e14-4f8e-8f3f-5f7e92c8e2ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3517: UserWarning: WARNING! min_tokens is not default parameter.\n",
      "                min_tokens was transferred to model_kwargs.\n",
      "                Please confirm that min_tokens is what you intended.\n",
      "  if await self.run_code(code, result, async_=asy):\n",
      "llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from granite-3.0-8b-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite 3.0 8b Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = granite-3.0\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,3]       = [\"language\", \"granite-3.0\", \"text-gen...\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                         granite.vocab_size u32              = 49155\n",
      "llama_model_loader: - kv  18:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  21:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  22:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  23:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49155]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|start_of_r...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/granite-3.0-8b-instruct-G...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 280\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 152\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  240 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.2826 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = granite\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49155\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 1.6e+01\n",
      "llm_load_print_meta: n_ff             = 12800\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.17 B\n",
      "llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = Granite 3.0 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: max token length = 512\n",
      "llm_load_print_meta: f_embedding_scale = 12.000000\n",
      "llm_load_print_meta: f_residual_scale  = 0.220000\n",
      "llm_load_print_meta: f_attention_scale = 0.007812\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4712.21 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =   640.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  640.00 MiB, K (f16):  320.00 MiB, V (f16):  320.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     5.00 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1368\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 1 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/granite-3.0-8b-instruct-GGUF/granite-3.0-8b-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if tools %}\\n    {{- '<|start_of_role|>available_tools<|end_of_role|>\\n' }}\\n    {%- for tool in tools %}\\n    {{- tool | tojson(indent=4) }}\\n    {%- if not loop.last %}\\n        {{- '\\n\\n' }}\\n    {%- endif %}\\n    {%- endfor %}\\n    {{- '<|end_of_text|>\\n' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'user' %}\\n    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant_tool_call' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'tool_response' %}\\n    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'tokenizer.ggml.model': 'gpt2', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.unknown_token_id': '0', 'granite.residual_scale': '0.220000', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.pre': 'refact', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '0', 'granite.vocab_size': '49155', 'general.basename': 'granite-3.0', 'quantize.imatrix.entries_count': '280', 'granite.block_count': '40', 'tokenizer.ggml.eos_token_id': '0', 'granite.attention.head_count': '32', 'quantize.imatrix.chunks_count': '152', 'tokenizer.ggml.add_bos_token': 'false', 'general.name': 'Granite 3.0 8b Instruct', 'general.size_label': '8B', 'general.finetune': 'instruct', 'general.type': 'model', 'granite.embedding_scale': '12.000000', 'general.license': 'apache-2.0', 'granite.attention.scale': '0.007812', 'granite.context_length': '4096', 'granite.attention.head_count_kv': '8', 'granite.rope.freq_base': '10000.000000', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'granite', 'granite.embedding_length': '4096', 'granite.feed_forward_length': '12800', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|start_of_role|>available_tools<|end_of_role|>\n",
      "' }}\n",
      "    {%- for tool in tools %}\n",
      "    {{- tool | tojson(indent=4) }}\n",
      "    {%- if not loop.last %}\n",
      "        {{- '\n",
      "\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'user' %}\n",
      "    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant_tool_call' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'tool_response' %}\n",
      "    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- if loop.last and add_generation_prompt %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = LlamaCpp(\n",
    "    model_path=\"granite-3.0-8b-instruct-Q4_K_M.gguf\", \n",
    "    temperature=0,\n",
    "    min_tokens=5,\n",
    "    max_tokens=250,\n",
    "    n_ctx=0,\n",
    "    stop=[\"Human:\", \"Observation\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22828a15-2c9f-49ae-931d-28b3db7c4bc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "template = \"Answer the {query} accurately. If you do not know the answer, simply say you do not know.\"\n",
    "prompt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7f86308-15c3-41c2-a727-5bbf39983f2f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "agent = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f3dbb74-c9d7-4523-894c-da0c2676f82b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    30 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    15 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    6988.05 ms /    45 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe sport played at the US Open is tennis.'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": \"What sport is played at the US Open?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "622b54d6-da1f-4870-b1ba-c738963d94d4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 2 prefix-match hit, remaining 36 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    36 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    57 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   20629.29 ms /    93 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe 2024 US Open Tennis Championship has not been announced yet, so the location is not known. Therefore, I do not know where the 2024 US Open Tennis Championship will be held.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.invoke({\"query\": \"Where was the 2024 US Open Tennis Championship?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de2cb276-4e32-4d10-a0b8-48c7f1fe4e33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "urls = [\n",
    "    \"https://www.ibm.com/case-studies/us-open\",\n",
    "    \"https://www.ibm.com/sports/usopen\",\n",
    "    \"https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement\",\n",
    "    \"https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edddfcb1-c3e9-4565-bbbd-e61c883fea3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en'}, page_content='\\n\\n\\n\\n\\n\\n\\n\\n\\nU.S. Open | IBM\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nHome\\n\\n\\n\\n\\nCase Studies\\n\\n\\n\\nUS Open \\n\\n\\n\\n\\n                \\n\\n\\n\\n  \\n    Acing the US Open digital experience\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n            \\n\\n                    \\n\\n\\n  \\n  \\n      AI models built with watsonx transform data into insight\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                \\n\\n\\nGet the latest AI and tech insights\\n\\n\\n\\nLearn More\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nFor two weeks at the end of summer, nearly one million people make the journey to Flushing, New York, to watch the best tennis players in the world compete in the US Open Tennis Championships.\\nYear after year, it is one of the most highly attended sporting events in the world.\\n\\nBut more than 15 million global tennis fans follow the tournament through the US Open app and website. And to keep them coming back for more, the United States Tennis Association (USTA) has worked side-by-side with IBM Consulting® for more than three decades, developing and delivering a world-class digital experience that constantly advances its features and functionality.\\n\\n“The digital experience of the US Open is of enormous importance to our global fans, and therefore to us,” says Kirsten Corio, Chief Commercial Officer at the USTA. “That means we need to constantly innovate to meet the modern demands of tennis fans, anticipating their needs, but also surprising them with new and unexpected experiences.”\\nTo help the US Open stay on the cutting edge of customer experience, IBM Consulting worked closely with the USTA to develop generative AI models that transform tennis data into insights and original content on the US Open app and website. To do this, the USTA used IBM® watsonx™, an AI and data platform built for business, and powerful AI models, including IBM Granite™ foundation models, to help develop key app features, such as Match Reports and AI Commentary for US Open highlight reels.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      15M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nWorld-class digital experiences for more than 15 million fans around the globe\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n  \\n  \\n      7M\\n  \\n\\n\\n\\n\\n    \\n\\n\\n\\n\\n\\n\\n\\nIBM captures and analyzes more than 7 million data points throughout the tournament\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      The AI models built with watsonx do more than enhance the digital experience of the US Open. They also scale the productivity of our editorial team by automating key workflows.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Generative AI experiences, built with watsonx\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nThe US Open is a sprawling, two-week tournament, with hundreds of matches played on 22 different courts. Keeping up with all the action is a challenge, both for tennis fans and the USTA editorial team covering the event. So, the USTA asked IBM to design, develop, and deliver solutions that enhance the digital experience and help its team serve up more content, covering more matches throughout the tournament.\\nTo do it, the IBM Consulting team built generative AI-powered features using the watsonx AI and data platform. For example, Match Reports are AI-generated post-match summaries that are designed to get fans quickly up to speed on the action from around the tournament. AI Commentary adds AI-generated, spoken commentary to match highlights. And SlamTracker–the premier scoring application for the US Open–features AI-generated match previews and recaps.\\n“The AI models built with watsonx do more than enhance the digital experience of the US Open,” says Kirsten Corio, Chief Commercial Officer, USTA. “They also scale the productivity of our editorial team by automating key workflows.”\\nThe IBM team worked with multiple AI models to develop the new features, including the family of Granit AI models. These large language models already understand language, but they needed to be trained, or “tuned,” on tennis data in order to translate US Open action into sentences and summaries.\\n“Foundation models are incredibly powerful and are ushering in a new age of generative AI,” says Shannon Miller, a Partner at IBM Consulting. “But to generate meaningful business outcomes, they need to be trained on high-quality data and develop domain expertise. And that’s why an organization’s proprietary data is the key differentiator when it comes to AI.”\\nThe team used watsonx.data to connect and curate the USTA’s trusted data sources. The curation process includes de-duping and filtering the foundational data that informs the large language model, as well as the USTA’s proprietary data. The process filters for things like profanity or abusive language and objectionable content.\\nThe models were then trained to translate tennis data into cogent descriptions, summarizing entire matches in the case of Match Reports, or generating sentences that describe the action in highlight reels for AI Commentary. Over the course of the 2024 US Open, Match Reports and AI Commentary will be generated for all men’s and women’s singles matches; something the USTA editorial team has never done before. And the ongoing operation of the models is monitored and managed using elements of watsonx.governance, which ensures the AI is performant, compliant and operating as expected.\\nDuring the software development phase of the project, the team took advantage of a powerful generative AI assistant to increase the efficiency and accuracy of its code. IBM watsonx Code Assistant™ uses generative AI from a purpose-built Granite model to accelerate software development, helping developers generate code based on natural language prompts. The team used this tool to analyze and explain snippets of code, annotate code to facilitate better collaboration between developers, and auto-complete snippets of analyzed code.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Platform of innovation\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nTo develop new capabilities every year, the USTA needs to move with speed and purpose. The process starts the week after the US Open concludes, when IBM Consulting kicks off work using the IBM Garage™ Methodology, a highly collaborative approach to co-creation.\\n“When we engage with a client, it’s critical that we work closely together every step of the way, ideating, iterating and adapting as we drive toward the client’s desired end state,” says Miller.\\nIn order to transform new ideas into digital reality, IBM Consulting designs, develops, and manages a powerful digital infrastructure capable of processing structured and unstructured data, and integrating technology from a variety of sources. This foundational infrastructure is advanced and improved upon every year.\\n“It used to be that innovation cycles were measured in years,” says the USTA’s Corio. “But now, innovation is measured in weeks and days, and it can come from anywhere. So, we needed a flexible platform that could handle all kinds of data, automate the process of turning data into insight, and do it all while protecting the entire digital environment.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    From data to insight\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nThe raw material of any digital experience is data, and the US Open tournament produces a lot of it. For starters, each US Open consists of 128 men and 128 women singles players, and a total of seven rounds for each tournament. Each tennis player comes with his or her own data set, including world ranking and recent performance. But that’s just the beginning.\\nOver the course of the tournament, more than 125,000 points will be played. And each one of those points generates its own data set: serve direction, speed, return shot type, winner shot type, rally count and even ball position. All told, more than seven million data points are generated during the tournament.\\nTo add more texture and context to the US Open digital experience, the team wanted to go beyond the numbers. So, they used AI to analyze the language and sentiment of millions of articles from hundreds of thousands of different sources to develop insights that are unique and informative, for instance, the likelihood to Win predictions for all singles matches. To help manage the collection, integration, and analysis of the data sets, IBM used IBM watsonx.data™, a purpose-built data store specifically designed to handle AI workloads.\\n“It’s a massive data management operation, incorporating multiple sources of data and a variety of partners,” says Miller. “But the magic happens when you combine hard data like stats and scores with unstructured data like media commentary. That is what gives tennis fans a more complete picture of each match.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    Automation, containerization and other efficiencies\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nTo streamline this process, during the years working with the UTSA, IBM Consulting built automated workflows that integrate and orchestrate the flow of data through the various applications and AI models needed to produce the digital experience. These workflows are made possible by a hybrid cloud architecture and the containerized apps running on Red Hat®\\xa0OpenShift®\\xa0on IBM Cloud.\\xa0The US Open hybrid multicloud architecture is made up of four public clouds, drawing on data from a variety of sources and integrating features and capability from a variety of partners. By containerizing the applications, the team can write them once and run them anywhere, ensuring that the right data gets to the right application on the right cloud.\\n\\n“With this platform, we’re capable of doing things that were not possible just a few years ago,” says Corio. “Managing all that data, producing AI-generated insights, securing the environment … IBM just makes it all come together for us. And I can’t wait to see what the future of the partnership holds.”\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n        \\n            \\n                \\n                     \\n                        \\n\\n\\n  \\n  \\n      Managing all that data, producing AI-generated insights, securing the environment…IBM just makes it all come together for us. And I can’t wait to see what the future of the partnership holds.\\n  \\n\\n\\n\\n\\n    \\n\\n\\n                        \\n                \\n            \\n        \\n        \\n            Kirsten Corio\\n        \\n\\n            Chief Commercial Officer\\n        \\n\\n            United States Tennis Association\\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n            \\n                \\n                    \\n\\n     \\n    About United States Tennis Association (USTA)\\n\\n\\n\\n\\n    \\n\\n\\n                \\n                \\n            \\n        \\n\\n\\n\\n\\nFounded in 1881, the USTAis the national governing body for the sport of tennis in the US. The US Open s the association’s Grand Slam tournament, first held in 1968—the year that Arthur Ashe won the men’s singles title. The US Open is played each September at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.\\n\\n\\n\\n\\n\\n\\n            \\n\\n\\n  \\n  \\n      Solution components\\n  \\n\\n\\n\\n\\n    \\n\\n\\n        \\n\\n        IBM Consulting™\\n        \\n        \\n    \\n\\n        IBM Garage™ Methodology\\n        \\n        \\n    \\n\\n        IBM® Granite™\\n        \\n        \\n    \\n\\n        IBM watsonx™\\n        \\n        \\n    \\n\\n        IBM watsonx.data™\\n        \\n        \\n    \\n\\n        IBM watsonx Code Assistant™\\n        \\n        \\n    \\n\\n        Red Hat® OpenShift® on IBM Cloud\\n        \\n        \\n    \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM watsonx\\n\\n\\n\\n\\nWant AI to make smart use of all your data? Use IBM watsonx to accelerate the fine tuning and deployment of your models.\\n\\n\\n\\n\\n\\nLearn more about watsonx\\n\\n\\nView more case stories\\n\\n\\n\\n\\n\\n\\n\\n        \\n\\n\\n\\n  \\n    The Masters\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nWhat if the Masters could turn data into insight?\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wimbledon\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nIBM and Wimbledon, a partnership of innovation\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n        \\n\\n\\n\\n  \\n    Wintershall Dea AG\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n    \\n\\n\\n\\nDrilling down into data to transform the oil and gas industry\\n\\n\\n\\nRead the case study\\n            \\n        \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n                            \\n\\n\\n\\n  \\n    Legal\\n\\n\\n\\n\\n\\n\\n    \\n\\n\\n                        \\n\\n\\n\\n\\n\\n© Copyright IBM Corporation 2023. IBM Corporation, IBM Consulting, New Orchard Road, Armonk, NY 10504\\nProduced in the United States of America, August 2024.\\nIBM, the IBM logo, ibm.com, IBM Consulting, IBM Garage, Granite, watsonx, watsonx.data, and Code Assistant are trademarks or registered trademarks of International Business Machines Corporation, in the United States and/or other countries. Other product and service names might be trademarks of IBM or other companies. A current list of IBM trademarks is available on ibm.com/trademark.\\n\\nRed Hat® and OpenShift® are registered trademarks of Red Hat, Inc. or its subsidiaries in the United States and other countries.\\xa0\\n\\nThis document is current as of the initial date of publication and may be changed by IBM at any time. Not all offerings are available in every country in which IBM operates.\\nAll client examples cited or described are presented as illustrations of the manner in which some clients have used IBM products and the results they may have achieved. Actual environmental costs and performance characteristics will vary depending on individual client configurations and conditions. Generally expected results cannot be provided as each client\\'s results will depend entirely on the client\\'s systems and services ordered. THE INFORMATION IN THIS DOCUMENT IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTY, EXPRESS OR IMPLIED, INCLUDING WITHOUT ANY WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND ANY WARRANTY OR CONDITION OF NON-INFRINGEMENT. IBM products are warranted according to the terms and conditions of the agreements under which they are provided.\\nStatement of Good Security Practices: No IT system or product should be considered completely secure, and no single product, service or security measure can be completely effective in preventing improper use or access.\\xa0 IBM does not warrant that any systems, products or services are immune from, or will make your enterprise immune from, the malicious or illegal conduct of any party.\\nThe client is responsible for ensuring compliance with all applicable laws and regulations. IBM does not provide legal advice nor represent or warrant that its services or products will ensure that the client is compliant with any law or regulation.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = [WebBaseLoader(url).load() for url in urls]\n",
    "docs_list = [item for sublist in docs for item in sublist]\n",
    "docs_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "475cf356-82a5-4012-8506-2778119ab2e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "connections.connect(host=MILVUS_HOST, port=MILVUS_PORT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f4bfe9a4-f9e3-4477-80bf-fc0f5bb5864b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=250, chunk_overlap=0)\n",
    "doc_splits = text_splitter.split_documents(docs_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "873c10cf-a38f-43d2-b597-9a12e92cf911",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_43/3401734470.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da4a7ccd98d4444a9ce9c3e414c37045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78a760d0d5ba438d94a9c0cf289f0828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63e7f464831d416a9178c76269e16255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6f88829a0b40d49a7c9b8b146d180b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "380a2b533a744d0fb2d3d8e545801503",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "174a51376f794136b9b081a4d4125aeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4babebcba404d7fbb6d6690c2f0b999",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0153a5383ff1472388a8495945e7fef0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a55e1280312a4aa5a93ac9ff19ff2a19",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eda5a08568e04b1285b64830af807f2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "412dd7d9478840ddbcc03ce464f85491",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embeddings = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "72910c5b-a76b-4cb9-83f3-69a57ee795c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "vector_store = Milvus.from_documents(\n",
    "            doc_splits,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"agentic_rag\",\n",
    "            connection_args={\"host\": MILVUS_HOST, \"port\": MILVUS_PORT}\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b985018-99a3-4c40-b94c-0bbf85c15104",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "retriever = vector_store.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ce42f606-918a-4242-9baa-3e65e2190ec6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_IBM_US_Open_context(question: str):\n",
    "    \"\"\"Get context about IBM's involvement in the 2024 US Open Tennis Championship.\"\"\"\n",
    "    context = retriever.invoke(question)\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "545bfe46-415d-4843-bcfb-c6bcc6b56b93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tools = [get_IBM_US_Open_context]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5dd5797c-9b58-4b20-a998-e06cd95effb5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\n",
    "Use a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\n",
    "Valid \"action\" values: \"Final Answer\" or {tool_names}\n",
    "Provide only ONE action per $JSON_BLOB, as shown:\"\n",
    "```\n",
    "{{\n",
    "  \"action\": $TOOL_NAME,\n",
    "  \"action_input\": $INPUT\n",
    "}}\n",
    "```\n",
    "Follow this format:\n",
    "Question: input question to answer\n",
    "Thought: consider previous and subsequent steps\n",
    "Action:\n",
    "```\n",
    "$JSON_BLOB\n",
    "```\n",
    "Observation: action result\n",
    "... (repeat Thought/Action/Observation N times)\n",
    "Thought: I know what to respond\n",
    "Action:\n",
    "```\n",
    "{{\n",
    "  \"action\": \"Final Answer\",\n",
    "  \"action_input\": \"Final response to human\"\n",
    "}}\n",
    "Begin! Reminder to ALWAYS respond with a valid json blob of a single action.\n",
    "Respond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b141e32e-99dd-4bbb-9997-7baae1c25c50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "human_prompt = \"\"\"{input}\n",
    "{agent_scratchpad}\n",
    "(reminder to always respond in a JSON blob)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "66bcd6d7-27db-401c-b9f3-2ff9460bed10",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\", optional=True),\n",
    "        (\"human\", human_prompt),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2de2d9ff-2730-46ba-bd3b-c70e75fdf78e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input', 'tool_names', 'tools'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7ffd3c161080>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': []}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], input_types={}, partial_variables={}, template='Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\nProvide only ONE action per $JSON_BLOB, as shown:\"\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\nFollow this format:\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={}, template='{input}\\n{agent_scratchpad}\\n(reminder to always respond in a JSON blob)'), additional_kwargs={})])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "cae24c84-b174-4f9a-b941-0ede5560d511",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt = prompt.partial(\n",
    "            tools=render_text_description_and_args(list(tools)),\n",
    "            tool_names=\", \".join([t.name for t in tools]),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "99d28481-78f6-46b1-a2df-b1f784bec8cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['agent_scratchpad', 'input'], optional_variables=['chat_history'], input_types={'chat_history': list[typing.Annotated[typing.Union[typing.Annotated[langchain_core.messages.ai.AIMessage, Tag(tag='ai')], typing.Annotated[langchain_core.messages.human.HumanMessage, Tag(tag='human')], typing.Annotated[langchain_core.messages.chat.ChatMessage, Tag(tag='chat')], typing.Annotated[langchain_core.messages.system.SystemMessage, Tag(tag='system')], typing.Annotated[langchain_core.messages.function.FunctionMessage, Tag(tag='function')], typing.Annotated[langchain_core.messages.tool.ToolMessage, Tag(tag='tool')], typing.Annotated[langchain_core.messages.ai.AIMessageChunk, Tag(tag='AIMessageChunk')], typing.Annotated[langchain_core.messages.human.HumanMessageChunk, Tag(tag='HumanMessageChunk')], typing.Annotated[langchain_core.messages.chat.ChatMessageChunk, Tag(tag='ChatMessageChunk')], typing.Annotated[langchain_core.messages.system.SystemMessageChunk, Tag(tag='SystemMessageChunk')], typing.Annotated[langchain_core.messages.function.FunctionMessageChunk, Tag(tag='FunctionMessageChunk')], typing.Annotated[langchain_core.messages.tool.ToolMessageChunk, Tag(tag='ToolMessageChunk')]], FieldInfo(annotation=NoneType, required=True, discriminator=Discriminator(discriminator=<function _get_type at 0x7ffd3c161080>, custom_error_type=None, custom_error_message=None, custom_error_context=None))]]}, partial_variables={'chat_history': [], 'tools': \"get_IBM_US_Open_context(question: str) - Get context about IBM's involvement in the 2024 US Open Tennis Championship., args: {'question': {'title': 'Question', 'type': 'string'}}\", 'tool_names': 'get_IBM_US_Open_context'}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=['tool_names', 'tools'], input_types={}, partial_variables={}, template='Respond to the human as helpfully and accurately as possible. You have access to the following tools: {tools}\\nUse a json blob to specify a tool by providing an action key (tool name) and an action_input key (tool input).\\nValid \"action\" values: \"Final Answer\" or {tool_names}\\nProvide only ONE action per $JSON_BLOB, as shown:\"\\n```\\n{{\\n  \"action\": $TOOL_NAME,\\n  \"action_input\": $INPUT\\n}}\\n```\\nFollow this format:\\nQuestion: input question to answer\\nThought: consider previous and subsequent steps\\nAction:\\n```\\n$JSON_BLOB\\n```\\nObservation: action result\\n... (repeat Thought/Action/Observation N times)\\nThought: I know what to respond\\nAction:\\n```\\n{{\\n  \"action\": \"Final Answer\",\\n  \"action_input\": \"Final response to human\"\\n}}\\nBegin! Reminder to ALWAYS respond with a valid json blob of a single action.\\nRespond directly if appropriate. Format is Action:```$JSON_BLOB```then Observation'), additional_kwargs={}), MessagesPlaceholder(variable_name='chat_history', optional=True), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['agent_scratchpad', 'input'], input_types={}, partial_variables={}, template='{input}\\n{agent_scratchpad}\\n(reminder to always respond in a JSON blob)'), additional_kwargs={})])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2af12f62-ca69-4442-8aa1-b22bf2efe694",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "558a61d0-96c2-4e5a-aa40-1d30ea9b24bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chain = (\n",
    "    RunnablePassthrough.assign(\n",
    "        agent_scratchpad=lambda x: format_log_to_str(x[\"intermediate_steps\"]),\n",
    "        chat_history=lambda x: memory.chat_memory.messages,\n",
    "    )\n",
    "    | prompt\n",
    "    | llm\n",
    "    | JSONAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=chain, tools=tools, handle_parsing_errors=True, verbose=True, memory=memory\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ff5bccbf-7d19-42f1-8b39-6c1414aac61f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   327 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   65692.77 ms /   426 tokens\n",
      "Llama.generate: 317 prefix-match hit, remaining 719 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Thought: The human asked about the location of the 2024 US Open Tennis Championship. I need to find out where it will be held.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_IBM_US_Open_context\",\n",
      "  \"action_input\": {\n",
      "    \"question\": \"Where will the 2024 US Open Tennis Championship be held?\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans ', 'description': '', 'language': 'en-us', 'pk': 455129936176156242}, page_content='As the US Open gets underway in New York this week, the world’s top tennis players are performing in front of empty seats—not the 800,000 or so fans who would normally fill the Billie Jean King National Tennis Center during the two-week tournament.'), Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'pk': 455129936176156137}, page_content='For two weeks at the end of summer, nearly one million people make the journey to Flushing, New York, to watch the best tennis players in the world compete in the US Open Tennis Championships.'), Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'pk': 455129936176156195}, page_content='September at the USTA Billie Jean King National Tennis Center in Flushing, Queens, New York.'), Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'pk': 455129936176156141}, page_content='“The digital experience of the US Open is of enormous importance to our global fans, and therefore to us,” says Kirsten Corio, Chief Commercial Officer at the USTA. “That means we need to constantly innovate to meet the modern demands of tennis')]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   719 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    61 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  147063.61 ms /   780 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": {\n",
      "    \"answer\": \"The 2024 US Open Tennis Championship will be held in Flushing, Queens, New York.\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for AIMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value={'answer': 'The 2024 US O...ing, Queens, New York.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]]\n  Input should be a valid list [type=list_type, input_value={'answer': 'The 2024 US O...ing, Queens, New York.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWhere was the 2024 US Open Tennis Championship?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/memory/chat_memory.py:74\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_input_output(inputs, outputs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_messages(\n\u001b[0;32m---> 74\u001b[0m     [HumanMessage(content\u001b[38;5;241m=\u001b[39minput_str), \u001b[43mAIMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_str\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/messages/ai.py:179\u001b[0m, in \u001b[0;36mAIMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m        content: The content of the message.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        kwargs: Additional arguments to pass to the parent class.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/messages/base.py:76\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m        kwargs: Additional fields to pass to the\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for AIMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value={'answer': 'The 2024 US O...ing, Queens, New York.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]]\n  Input should be a valid list [type=list_type, input_value={'answer': 'The 2024 US O...ing, Queens, New York.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"Where was the 2024 US Open Tennis Championship?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "523d8633-c627-4f01-98c6-7b77fac48446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 305 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    49 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    99 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   31178.98 ms /   148 tokens\n",
      "Llama.generate: 344 prefix-match hit, remaining 1000 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "AI: Thought: I need to find out how IBM used watsonx at the 2024 US Open Tennis Championship.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"get_IBM_US_Open_context\",\n",
      "  \"action_input\": {\n",
      "    \"question\": \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\"\n",
      "  }\n",
      "}\n",
      "```\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3m[Document(metadata={'source': 'https://newsroom.ibm.com/2024-08-15-ibm-and-the-usta-serve-up-new-and-enhanced-generative-ai-features-for-2024-us-open-digital-platforms', 'title': 'IBM and the USTA Serve Up New and Enhanced Generative AI Features for 2024 US Open Digital Platforms', 'description': \"IBM and the United States Tennis Association (USTA) announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and enhanced capabilities – a product of collaboration between IBM and the USTA digital team – aim to deliver a more informative and engaging experience for millions of tennis fans around the world.\", 'language': 'en-us', 'pk': 455129936176156303}, page_content=\"ARMONK, N.Y., Aug. 15, 2024 /PRNewswire/ -- IBM (NYSE:\\xa0IBM) and the United States Tennis Association (USTA) today announced several watsonx-powered fan features coming to the US Open digital platforms ahead of this year's tournament. These new and\"), Document(metadata={'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans ', 'description': '', 'language': 'en-us', 'pk': 455129936176156251}, page_content='development on the platforms, the team had to halt and pivot. Only in mid-June did the IBM team learn that US Open would become the first major tennis tournament to be held since the pandemic disrupted professional sports. And only then did the'), Document(metadata={'source': 'https://www.ibm.com/case-studies/us-open', 'title': 'U.S. Open | IBM', 'description': 'To help the US Open stay on the cutting edge of customer experience, IBM Consulting built powerful generative AI models with watsonx.', 'language': 'en', 'pk': 455129936176156150}, page_content='The US Open is a sprawling, two-week tournament, with hundreds of matches played on 22 different courts. Keeping up with all the action is a challenge, both for tennis fans and the USTA editorial team covering the event. So, the USTA asked IBM to'), Document(metadata={'source': 'https://newsroom.ibm.com/US-Open-AI-Tennis-Fan-Engagement', 'title': 'US Open: IBM and AI Can Help Engage Tennis Fans ', 'description': '', 'language': 'en-us', 'pk': 455129936176156238}, page_content='US Open: IBM and AI Can Help Engage Tennis Fans \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM Logo\\n\\n\\n\\n Submit\\n \\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nIBM\\xa0Newsroom\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nNews')]\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /  1000 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /   100 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  292026.20 ms /  1100 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": {\n",
      "    \"answer\": \"IBM used watsonx at the 2024 US Open Tennis Championship to power new and enhanced fan features on the digital platforms. These features were developed by IBM in collaboration with the USTA digital team to deliver a more informative and engaging experience for tennis fans around the world.\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "2 validation errors for AIMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value={'answer': 'IBM used wats...fans around the world.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]]\n  Input should be a valid list [type=list_type, input_value={'answer': 'IBM used wats...fans around the world.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[54], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHow did IBM use watsonx at the 2024 US Open Tennis Championship?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:170\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    171\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m include_run_info:\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:165\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_inputs(inputs)\n\u001b[1;32m    159\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    160\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs, run_manager\u001b[38;5;241m=\u001b[39mrun_manager)\n\u001b[1;32m    161\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    163\u001b[0m     )\n\u001b[0;32m--> 165\u001b[0m     final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprep_outputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    169\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/chains/base.py:466\u001b[0m, in \u001b[0;36mChain.prep_outputs\u001b[0;34m(self, inputs, outputs, return_only_outputs)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_outputs(outputs)\n\u001b[1;32m    465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 466\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmemory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    467\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_only_outputs:\n\u001b[1;32m    468\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain/memory/chat_memory.py:74\u001b[0m, in \u001b[0;36mBaseChatMemory.save_context\u001b[0;34m(self, inputs, outputs)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Save context from this conversation to buffer.\"\"\"\u001b[39;00m\n\u001b[1;32m     72\u001b[0m input_str, output_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_input_output(inputs, outputs)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchat_memory\u001b[38;5;241m.\u001b[39madd_messages(\n\u001b[0;32m---> 74\u001b[0m     [HumanMessage(content\u001b[38;5;241m=\u001b[39minput_str), \u001b[43mAIMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_str\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m     75\u001b[0m )\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/messages/ai.py:179\u001b[0m, in \u001b[0;36mAIMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    171\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m    172\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    173\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \n\u001b[1;32m    175\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;124;03m        content: The content of the message.\u001b[39;00m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        kwargs: Additional arguments to pass to the parent class.\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/messages/base.py:76\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[0;34m(self, content, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[1;32m     69\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m        kwargs: Additional fields to pass to the\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 76\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/langchain_core/load/serializable.py:125\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/app-root/lib64/python3.11/site-packages/pydantic/main.py:214\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[39;00m\n\u001b[1;32m    213\u001b[0m __tracebackhide__ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m validated_self \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_self:\n\u001b[1;32m    216\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    217\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA custom validator is returning a value other than `self`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReturning anything other than `self` from a top level model validator isn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt supported when validating via `__init__`.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSee the `model_validator` docs (https://docs.pydantic.dev/latest/concepts/validators/#model-validators) for more details.\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    220\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    221\u001b[0m     )\n",
      "\u001b[0;31mValidationError\u001b[0m: 2 validation errors for AIMessage\ncontent.str\n  Input should be a valid string [type=string_type, input_value={'answer': 'IBM used wats...fans around the world.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/string_type\ncontent.list[union[str,dict[any,any]]]\n  Input should be a valid list [type=list_type, input_value={'answer': 'IBM used wats...fans around the world.'}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.10/v/list_type"
     ]
    }
   ],
   "source": [
    "agent_executor.invoke(\n",
    "    {\"input\": \"How did IBM use watsonx at the 2024 US Open Tennis Championship?\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c7fa29e4-3e86-4ec1-80da-8378b4266acb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 297 prefix-match hit, remaining 19 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    4380.59 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /    19 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    62 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =   17705.46 ms /    81 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3m\n",
      "Assistant:\n",
      "Thought: The user is asking for the capital of France. I don't need to use any tools for this information.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"The capital of France is Paris.\"\n",
      "}\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'What is the capital of France?',\n",
       " 'history': '',\n",
       " 'output': 'The capital of France is Paris.'}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_executor.invoke({\"input\": \"What is the capital of France?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7975ca-11d0-4797-9e5e-b5573fd96803",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
