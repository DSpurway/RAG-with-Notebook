{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df98e6b8-3eec-46d5-a22a-06ff42585fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "import os\n",
    "import ast\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.utils import get_json_schema\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(os.getcwd()+\"/.env\", override=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2dfba063-2aa1-4dc1-afde-fc933b5d39de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ac1a20d50c64ed6ab3ffb83ea1bcf1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/5.64k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a24011673ee8484ea39c8f9a405d8613",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4599447940744e3a67b86b3ce956c41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5f9cc0fc30647e9ac1dfff8bb295988",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/3.48M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03dfd804c9d64508ad49f8826fa7a4e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/87.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "465f586d3e6145d28795b775abc6a2c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/701 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "TOKENIZER = AutoTokenizer.from_pretrained(\"ibm-granite/granite-3.0-8b-instruct\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05037ec2-2ce3-4fe9-b35a-8500139f303d",
   "metadata": {},
   "outputs": [],
   "source": [
    "FNAME = \"granite-3.0-8b-instruct-Q4_K_M.gguf\"\n",
    "\n",
    "if not os.path.exists(FNAME):\n",
    "    res = requests.get('https://huggingface.co/bartowski/granite-3.0-8b-instruct-GGUF/resolve/main/granite-3.0-8b-instruct-Q4_K_M.gguf')\n",
    "    with open(FNAME, 'wb') as file:\n",
    "        file.write(res.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73118fb0-fb87-41f3-8fbd-06c5dbd5957b",
   "metadata": {},
   "outputs": [],
   "source": [
    "AV_STOCK_API_KEY = os.getenv('AV_STOCK_API_KEY',\"TRIITLN2AW82X8LH\")\n",
    "WEATHER_API_KEY = os.getenv('WEATHER_API_KEY',\"e43e2793e6eccc0fabf9ca3d4cf0cb0f\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b710bf6-0881-41d3-98b1-e55d06cad223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_stock_price(ticker: str, date: str) -> dict:\n",
    "    \"\"\"\n",
    "    Retrieves the lowest and highest stock prices for a given ticker and date.\n",
    "    Args:\n",
    "    ticker: The stock ticker symbol, e.g., \"IBM\".\n",
    "    date: The date in \"YYYY-MM-DD\" format for which you want to get stock prices.\n",
    "    Returns:\n",
    "    A dictionary containing the low and high stock prices on the given date.\n",
    "    \"\"\"\n",
    "    print(f\"Getting stock price for {ticker} on {date}\")\n",
    "    try:\n",
    "        stock_url = f\"https://www.alphavantage.co/query?function=TIME_SERIES_DAILY&symbol={ticker}&apikey={AV_STOCK_API_KEY}\"\n",
    "        stock_data = requests.get(stock_url)\n",
    "        stock_low = stock_data.json()[\"Time Series (Daily)\"][date][\"3. low\"]\n",
    "        stock_high = stock_data.json()[\"Time Series (Daily)\"][date][\"2. high\"]\n",
    "        return {\n",
    "            \"low\": stock_low,\n",
    "            \"high\": stock_high\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching stock data: {e}\")\n",
    "        return {\n",
    "            \"low\": \"none\",\n",
    "            \"high\": \"none\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09807cc6-5158-45b6-b476-ff7dcc0554d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_current_weather(location: str) -> dict:\n",
    "    \"\"\"\n",
    "    Fetches the current weather for a given location (default: San Francisco).\n",
    "\n",
    "    Args:\n",
    "        location: The name of the city for which to retrieve the weather information.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing weather information such as temperature, weather description, and humidity.\n",
    "    \"\"\"\n",
    "    print(f\"Getting current weather for {location}\")\n",
    "\n",
    "    try:\n",
    "        # API request to fetch weather data\n",
    "        weather_url = f\"http://api.openweathermap.org/data/2.5/weather?q={location}&appid={WEATHER_API_KEY}&units=metric\"\n",
    "        weather_data = requests.get(weather_url)\n",
    "        data = weather_data.json()\n",
    "        # Extracting relevant weather details\n",
    "        weather_description = data[\"weather\"][0][\"description\"]\n",
    "        temperature = data[\"main\"][\"temp\"]\n",
    "        humidity = data[\"main\"][\"humidity\"]\n",
    "\n",
    "        # Returning weather details\n",
    "        return {\n",
    "            \"description\": weather_description,\n",
    "            \"temperature\": temperature,\n",
    "            \"humidity\": humidity\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching weather data: {e}\")\n",
    "        return {\n",
    "            \"description\": \"none\",\n",
    "            \"temperature\": \"none\",\n",
    "            \"humidity\": \"none\"\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5dd73bdf-89ce-4121-b250-b8697407d6cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_api_request(instructions: str) -> str:\n",
    "    model = LlamaCpp(\n",
    "        model_path=\"granite-3.0-8b-instruct-Q4_K_M.gguf\", \n",
    "        temperature=0,\n",
    "        min_tokens=5,\n",
    "        max_tokens=200,\n",
    "      stop=[TOKENIZER.eos_token]\n",
    "    )\n",
    "    response = model.invoke(instructions)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0865fe88-212d-4093-b254-9cd787dbbaea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'function': {'name': 'get_stock_price',\n",
       "   'description': 'Retrieves the lowest and highest stock prices for a given ticker and date.',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'ticker': {'type': 'string',\n",
       "      'description': 'The stock ticker symbol, e.g., \"IBM\".'},\n",
       "     'date': {'type': 'string',\n",
       "      'description': 'The date in \"YYYY-MM-DD\" format for which you want to get stock prices.'}},\n",
       "    'required': ['ticker', 'date']},\n",
       "   'return': {'type': 'object',\n",
       "    'description': 'A dictionary containing the low and high stock prices on the given date.'}}},\n",
       " {'type': 'function',\n",
       "  'function': {'name': 'get_current_weather',\n",
       "   'description': 'Fetches the current weather for a given location (default: San Francisco).',\n",
       "   'parameters': {'type': 'object',\n",
       "    'properties': {'location': {'type': 'string',\n",
       "      'description': 'The name of the city for which to retrieve the weather information.'}},\n",
       "    'required': ['location']},\n",
       "   'return': {'type': 'object',\n",
       "    'description': 'A dictionary containing weather information such as temperature, weather description, and humidity.'}}}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [get_json_schema(tool) for tool in (get_stock_price, get_current_weather)]\n",
    "tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "506d4821-3214-465b-8acc-3f8963e7d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What were the IBM stock prices on October 7, 2024?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b1b814d-95d1-425f-92eb-9e0b1ca33e84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_stock_price\",\\n        \"description\": \"Retrieves the lowest and highest stock prices for a given ticker and date.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"ticker\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The stock ticker symbol, e.g., \\\\\"IBM\\\\\".\"\\n                },\\n                \"date\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The date in \\\\\"YYYY-MM-DD\\\\\" format for which you want to get stock prices.\"\\n                }\\n            },\\n            \"required\": [\\n                \"ticker\",\\n                \"date\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing the low and high stock prices on the given date.\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_current_weather\",\\n        \"description\": \"Fetches the current weather for a given location (default: San Francisco).\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"location\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The name of the city for which to retrieve the weather information.\"\\n                }\\n            },\\n            \"required\": [\\n                \"location\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing weather information such as temperature, weather description, and humidity.\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What were the IBM stock prices on October 7, 2024?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},\n",
    "    {\"role\": \"user\", \"content\": query },\n",
    "]\n",
    "\n",
    "instruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "instruction_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dec1e77d-9792-4738-ba21-b148b4b154eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! min_tokens is not default parameter.\n",
      "                min_tokens was transferred to model_kwargs.\n",
      "                Please confirm that min_tokens is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from granite-3.0-8b-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite 3.0 8b Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = granite-3.0\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,3]       = [\"language\", \"granite-3.0\", \"text-gen...\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                         granite.vocab_size u32              = 49155\n",
      "llama_model_loader: - kv  18:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  21:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  22:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  23:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49155]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|start_of_r...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/granite-3.0-8b-instruct-G...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 280\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 152\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  240 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.2826 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = granite\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49155\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 1.6e+01\n",
      "llm_load_print_meta: n_ff             = 12800\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.17 B\n",
      "llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = Granite 3.0 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: max token length = 512\n",
      "llm_load_print_meta: f_embedding_scale = 12.000000\n",
      "llm_load_print_meta: f_residual_scale  = 0.220000\n",
      "llm_load_print_meta: f_attention_scale = 0.007812\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4712.21 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    80.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1368\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 1 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/granite-3.0-8b-instruct-GGUF/granite-3.0-8b-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if tools %}\\n    {{- '<|start_of_role|>available_tools<|end_of_role|>\\n' }}\\n    {%- for tool in tools %}\\n    {{- tool | tojson(indent=4) }}\\n    {%- if not loop.last %}\\n        {{- '\\n\\n' }}\\n    {%- endif %}\\n    {%- endfor %}\\n    {{- '<|end_of_text|>\\n' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'user' %}\\n    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant_tool_call' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'tool_response' %}\\n    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'tokenizer.ggml.model': 'gpt2', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.unknown_token_id': '0', 'granite.residual_scale': '0.220000', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.pre': 'refact', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '0', 'granite.vocab_size': '49155', 'general.basename': 'granite-3.0', 'quantize.imatrix.entries_count': '280', 'granite.block_count': '40', 'tokenizer.ggml.eos_token_id': '0', 'granite.attention.head_count': '32', 'quantize.imatrix.chunks_count': '152', 'tokenizer.ggml.add_bos_token': 'false', 'general.name': 'Granite 3.0 8b Instruct', 'general.size_label': '8B', 'general.finetune': 'instruct', 'general.type': 'model', 'granite.embedding_scale': '12.000000', 'general.license': 'apache-2.0', 'granite.attention.scale': '0.007812', 'granite.context_length': '4096', 'granite.attention.head_count_kv': '8', 'granite.rope.freq_base': '10000.000000', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'granite', 'granite.embedding_length': '4096', 'granite.feed_forward_length': '12800', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|start_of_role|>available_tools<|end_of_role|>\n",
      "' }}\n",
      "    {%- for tool in tools %}\n",
      "    {{- tool | tojson(indent=4) }}\n",
      "    {%- if not loop.last %}\n",
      "        {{- '\n",
      "\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'user' %}\n",
      "    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant_tool_call' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'tool_response' %}\n",
      "    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- if loop.last and add_generation_prompt %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n",
      "llama_perf_context_print:        load time =  113435.62 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   417 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    36 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  121430.22 ms /   453 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"name\": \"get_stock_price\", \"arguments\": {\"ticker\": \"IBM\", \"date\": \"2024-10-07\"}}]'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = make_api_request(instruction_1)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7d6811b9-c3c3-43cf-bc2c-14a20a1ba9e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tool_call(llm_response: str):\n",
    "    tool_request = ast.literal_eval(re.search(\"({.+})\", llm_response).group(0))\n",
    "    tool_name = tool_request[\"name\"]\n",
    "    tool_arguments = tool_request[\"arguments\"]\n",
    "    tool_response = globals()[tool_name](**tool_arguments)\n",
    "    return tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8fcbdcae-d532-4cd4-a505-140a7f2a4219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting stock price for IBM on 2024-10-07\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'low': '225.0200', 'high': '227.6700'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_response = tool_call(data_1)\n",
    "tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1dc38fa6-bb7a-407f-8f53-044a385b3418",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! min_tokens is not default parameter.\n",
      "                min_tokens was transferred to model_kwargs.\n",
      "                Please confirm that min_tokens is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from granite-3.0-8b-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite 3.0 8b Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = granite-3.0\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,3]       = [\"language\", \"granite-3.0\", \"text-gen...\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                         granite.vocab_size u32              = 49155\n",
      "llama_model_loader: - kv  18:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  21:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  22:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  23:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49155]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|start_of_r...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/granite-3.0-8b-instruct-G...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 280\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 152\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  240 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.2826 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = granite\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49155\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 1.6e+01\n",
      "llm_load_print_meta: n_ff             = 12800\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.17 B\n",
      "llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = Granite 3.0 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: max token length = 512\n",
      "llm_load_print_meta: f_embedding_scale = 12.000000\n",
      "llm_load_print_meta: f_residual_scale  = 0.220000\n",
      "llm_load_print_meta: f_attention_scale = 0.007812\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4712.21 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    80.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1368\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 1 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/granite-3.0-8b-instruct-GGUF/granite-3.0-8b-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if tools %}\\n    {{- '<|start_of_role|>available_tools<|end_of_role|>\\n' }}\\n    {%- for tool in tools %}\\n    {{- tool | tojson(indent=4) }}\\n    {%- if not loop.last %}\\n        {{- '\\n\\n' }}\\n    {%- endif %}\\n    {%- endfor %}\\n    {{- '<|end_of_text|>\\n' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'user' %}\\n    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant_tool_call' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'tool_response' %}\\n    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'tokenizer.ggml.model': 'gpt2', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.unknown_token_id': '0', 'granite.residual_scale': '0.220000', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.pre': 'refact', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '0', 'granite.vocab_size': '49155', 'general.basename': 'granite-3.0', 'quantize.imatrix.entries_count': '280', 'granite.block_count': '40', 'tokenizer.ggml.eos_token_id': '0', 'granite.attention.head_count': '32', 'quantize.imatrix.chunks_count': '152', 'tokenizer.ggml.add_bos_token': 'false', 'general.name': 'Granite 3.0 8b Instruct', 'general.size_label': '8B', 'general.finetune': 'instruct', 'general.type': 'model', 'granite.embedding_scale': '12.000000', 'general.license': 'apache-2.0', 'granite.attention.scale': '0.007812', 'granite.context_length': '4096', 'granite.attention.head_count_kv': '8', 'granite.rope.freq_base': '10000.000000', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'granite', 'granite.embedding_length': '4096', 'granite.feed_forward_length': '12800', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|start_of_role|>available_tools<|end_of_role|>\n",
      "' }}\n",
      "    {%- for tool in tools %}\n",
      "    {{- tool | tojson(indent=4) }}\n",
      "    {%- if not loop.last %}\n",
      "        {{- '\n",
      "\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'user' %}\n",
      "    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant_tool_call' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'tool_response' %}\n",
      "    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- if loop.last and add_generation_prompt %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n",
      "llama_perf_context_print:        load time =  119819.90 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   474 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    37 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  127378.54 ms /   511 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The IBM stock prices on October 7, 2024 were a low of $225.02 and a high of $227.67.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2 = conversation + [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },\n",
    "    {\"role\": \"tool_response\", \"content\": str(tool_response) },\n",
    "]\n",
    "\n",
    "instruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "data_2 = make_api_request(instruction_2)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8603a42a-da40-49f5-b3c3-8a7478c2bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"What is the current weather in Glasgow, UK?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e7ab3629-24e3-4dec-80e6-069c781ff299",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|start_of_role|>available_tools<|end_of_role|>\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_stock_price\",\\n        \"description\": \"Retrieves the lowest and highest stock prices for a given ticker and date.\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"ticker\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The stock ticker symbol, e.g., \\\\\"IBM\\\\\".\"\\n                },\\n                \"date\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The date in \\\\\"YYYY-MM-DD\\\\\" format for which you want to get stock prices.\"\\n                }\\n            },\\n            \"required\": [\\n                \"ticker\",\\n                \"date\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing the low and high stock prices on the given date.\"\\n        }\\n    }\\n}\\n\\n{\\n    \"type\": \"function\",\\n    \"function\": {\\n        \"name\": \"get_current_weather\",\\n        \"description\": \"Fetches the current weather for a given location (default: San Francisco).\",\\n        \"parameters\": {\\n            \"type\": \"object\",\\n            \"properties\": {\\n                \"location\": {\\n                    \"type\": \"string\",\\n                    \"description\": \"The name of the city for which to retrieve the weather information.\"\\n                }\\n            },\\n            \"required\": [\\n                \"location\"\\n            ]\\n        },\\n        \"return\": {\\n            \"type\": \"object\",\\n            \"description\": \"A dictionary containing weather information such as temperature, weather description, and humidity.\"\\n        }\\n    }\\n}<|end_of_text|>\\n<|start_of_role|>system<|end_of_role|>You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.<|end_of_text|>\\n<|start_of_role|>user<|end_of_role|>What is the current weather in Glasgow, UK?<|end_of_text|>\\n<|start_of_role|>assistant<|end_of_role|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation = [\n",
    "    {\"role\": \"system\",\"content\": \"You are a helpful assistant with access to the following function calls. Your task is to produce a list of function calls necessary to generate response to the user utterance. Use the following function calls as required.\"},\n",
    "    {\"role\": \"user\", \"content\": query },\n",
    "]\n",
    "\n",
    "instruction_1 = TOKENIZER.apply_chat_template(conversation=conversation, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "instruction_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e0daff7-acab-4d41-bd4c-3520d939ac14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! min_tokens is not default parameter.\n",
      "                min_tokens was transferred to model_kwargs.\n",
      "                Please confirm that min_tokens is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from granite-3.0-8b-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite 3.0 8b Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = granite-3.0\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,3]       = [\"language\", \"granite-3.0\", \"text-gen...\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                         granite.vocab_size u32              = 49155\n",
      "llama_model_loader: - kv  18:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  21:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  22:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  23:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49155]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|start_of_r...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/granite-3.0-8b-instruct-G...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 280\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 152\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  240 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.2826 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = granite\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49155\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 1.6e+01\n",
      "llm_load_print_meta: n_ff             = 12800\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.17 B\n",
      "llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = Granite 3.0 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: max token length = 512\n",
      "llm_load_print_meta: f_embedding_scale = 12.000000\n",
      "llm_load_print_meta: f_residual_scale  = 0.220000\n",
      "llm_load_print_meta: f_attention_scale = 0.007812\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4712.21 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    80.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1368\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 1 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/granite-3.0-8b-instruct-GGUF/granite-3.0-8b-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if tools %}\\n    {{- '<|start_of_role|>available_tools<|end_of_role|>\\n' }}\\n    {%- for tool in tools %}\\n    {{- tool | tojson(indent=4) }}\\n    {%- if not loop.last %}\\n        {{- '\\n\\n' }}\\n    {%- endif %}\\n    {%- endfor %}\\n    {{- '<|end_of_text|>\\n' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'user' %}\\n    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant_tool_call' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'tool_response' %}\\n    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'tokenizer.ggml.model': 'gpt2', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.unknown_token_id': '0', 'granite.residual_scale': '0.220000', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.pre': 'refact', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '0', 'granite.vocab_size': '49155', 'general.basename': 'granite-3.0', 'quantize.imatrix.entries_count': '280', 'granite.block_count': '40', 'tokenizer.ggml.eos_token_id': '0', 'granite.attention.head_count': '32', 'quantize.imatrix.chunks_count': '152', 'tokenizer.ggml.add_bos_token': 'false', 'general.name': 'Granite 3.0 8b Instruct', 'general.size_label': '8B', 'general.finetune': 'instruct', 'general.type': 'model', 'granite.embedding_scale': '12.000000', 'general.license': 'apache-2.0', 'granite.attention.scale': '0.007812', 'granite.context_length': '4096', 'granite.attention.head_count_kv': '8', 'granite.rope.freq_base': '10000.000000', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'granite', 'granite.embedding_length': '4096', 'granite.feed_forward_length': '12800', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|start_of_role|>available_tools<|end_of_role|>\n",
      "' }}\n",
      "    {%- for tool in tools %}\n",
      "    {{- tool | tojson(indent=4) }}\n",
      "    {%- if not loop.last %}\n",
      "        {{- '\n",
      "\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'user' %}\n",
      "    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant_tool_call' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'tool_response' %}\n",
      "    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- if loop.last and add_generation_prompt %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n",
      "llama_perf_context_print:        load time =  113593.94 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   413 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    26 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  119954.76 ms /   439 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'[{\"name\": \"get_current_weather\", \"arguments\": {\"location\": \"Glasgow, UK\"}}]'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_1 = make_api_request(instruction_1)\n",
    "data_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "08de7865-803c-4b7f-8582-3ead7c854cf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting current weather for Glasgow, UK\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'description': 'few clouds', 'temperature': 8.43, 'humidity': 76}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_response = tool_call(data_1)\n",
    "tool_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1881a9e5-eef3-45ae-b6a1-aa1aad01c778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/app-root/lib64/python3.11/site-packages/IPython/core/interactiveshell.py:3577: UserWarning: WARNING! min_tokens is not default parameter.\n",
      "                min_tokens was transferred to model_kwargs.\n",
      "                Please confirm that min_tokens is what you intended.\n",
      "  exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "llama_model_loader: loaded meta data with 40 key-value pairs and 362 tensors from granite-3.0-8b-instruct-Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = granite\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Granite 3.0 8b Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = granite-3.0\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = apache-2.0\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,3]       = [\"language\", \"granite-3.0\", \"text-gen...\n",
      "llama_model_loader: - kv   8:                        granite.block_count u32              = 40\n",
      "llama_model_loader: - kv   9:                     granite.context_length u32              = 4096\n",
      "llama_model_loader: - kv  10:                   granite.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  11:                granite.feed_forward_length u32              = 12800\n",
      "llama_model_loader: - kv  12:               granite.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  13:            granite.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  14:                     granite.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  15:   granite.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  16:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  17:                         granite.vocab_size u32              = 49155\n",
      "llama_model_loader: - kv  18:               granite.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  20:                    granite.attention.scale f32              = 0.007812\n",
      "llama_model_loader: - kv  21:                    granite.embedding_scale f32              = 12.000000\n",
      "llama_model_loader: - kv  22:                     granite.residual_scale f32              = 0.220000\n",
      "llama_model_loader: - kv  23:                        granite.logit_scale f32              = 16.000000\n",
      "llama_model_loader: - kv  24:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  25:                         tokenizer.ggml.pre str              = refact\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.tokens arr[str,49155]   = [\"<|end_of_text|>\", \"<fim_prefix>\", \"...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,49155]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  28:                      tokenizer.ggml.merges arr[str,48891]   = [\"Ġ Ġ\", \"ĠĠ ĠĠ\", \"ĠĠĠĠ ĠĠ...\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.bos_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:                tokenizer.ggml.eos_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  32:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {%- if tools %}\\n    {{- '<|start_of_r...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  36:                      quantize.imatrix.file str              = /models_out/granite-3.0-8b-instruct-G...\n",
      "llama_model_loader: - kv  37:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  38:             quantize.imatrix.entries_count i32              = 280\n",
      "llama_model_loader: - kv  39:              quantize.imatrix.chunks_count i32              = 152\n",
      "llama_model_loader: - type  f32:   81 tensors\n",
      "llama_model_loader: - type q4_K:  240 tensors\n",
      "llama_model_loader: - type q6_K:   41 tensors\n",
      "llm_load_vocab: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "llm_load_vocab: special tokens cache size = 22\n",
      "llm_load_vocab: token to piece cache size = 0.2826 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = granite\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 49155\n",
      "llm_load_print_meta: n_merges         = 48891\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_layer          = 40\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 8\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_swa            = 0\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 4\n",
      "llm_load_print_meta: n_embd_k_gqa     = 1024\n",
      "llm_load_print_meta: n_embd_v_gqa     = 1024\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 1.6e+01\n",
      "llm_load_print_meta: n_ff             = 12800\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 8.17 B\n",
      "llm_load_print_meta: model size       = 4.60 GiB (4.84 BPW) \n",
      "llm_load_print_meta: general.name     = Granite 3.0 8b Instruct\n",
      "llm_load_print_meta: BOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: EOS token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: PAD token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: LF token         = 145 'Ä'\n",
      "llm_load_print_meta: EOG token        = 0 '<|end_of_text|>'\n",
      "llm_load_print_meta: max token length = 512\n",
      "llm_load_print_meta: f_embedding_scale = 12.000000\n",
      "llm_load_print_meta: f_residual_scale  = 0.220000\n",
      "llm_load_print_meta: f_attention_scale = 0.007812\n",
      "llm_load_tensors: ggml ctx size =    0.17 MiB\n",
      "llm_load_tensors:        CPU buffer size =  4712.21 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 32\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 32\n",
      "llama_new_context_with_model: n_ubatch   = 8\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =    80.00 MiB\n",
      "llama_new_context_with_model: KV self size  =   80.00 MiB, K (f16):   40.00 MiB, V (f16):   40.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.19 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     1.63 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1368\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | AVX512_BF16 = 0 | FMA = 0 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 0 | FP16_VA = 0 | RISCV_VECT = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 0 | SSSE3 = 0 | VSX = 1 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.file': '/models_out/granite-3.0-8b-instruct-GGUF/granite-3.0-8b-instruct.imatrix', 'general.quantization_version': '2', 'tokenizer.chat_template': \"{%- if tools %}\\n    {{- '<|start_of_role|>available_tools<|end_of_role|>\\n' }}\\n    {%- for tool in tools %}\\n    {{- tool | tojson(indent=4) }}\\n    {%- if not loop.last %}\\n        {{- '\\n\\n' }}\\n    {%- endif %}\\n    {%- endfor %}\\n    {{- '<|end_of_text|>\\n' }}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'user' %}\\n    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'assistant_tool_call' %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- elif message['role'] == 'tool_response' %}\\n    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\\n' }}\\n    {%- endif %}\\n    {%- if loop.last and add_generation_prompt %}\\n    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\\n    {%- endif %}\\n{%- endfor %}\", 'tokenizer.ggml.model': 'gpt2', 'granite.logit_scale': '16.000000', 'tokenizer.ggml.unknown_token_id': '0', 'granite.residual_scale': '0.220000', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.pre': 'refact', 'tokenizer.ggml.add_space_prefix': 'false', 'granite.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '0', 'granite.vocab_size': '49155', 'general.basename': 'granite-3.0', 'quantize.imatrix.entries_count': '280', 'granite.block_count': '40', 'tokenizer.ggml.eos_token_id': '0', 'granite.attention.head_count': '32', 'quantize.imatrix.chunks_count': '152', 'tokenizer.ggml.add_bos_token': 'false', 'general.name': 'Granite 3.0 8b Instruct', 'general.size_label': '8B', 'general.finetune': 'instruct', 'general.type': 'model', 'granite.embedding_scale': '12.000000', 'general.license': 'apache-2.0', 'granite.attention.scale': '0.007812', 'granite.context_length': '4096', 'granite.attention.head_count_kv': '8', 'granite.rope.freq_base': '10000.000000', 'granite.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'granite', 'granite.embedding_length': '4096', 'granite.feed_forward_length': '12800', 'general.file_type': '15'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {%- if tools %}\n",
      "    {{- '<|start_of_role|>available_tools<|end_of_role|>\n",
      "' }}\n",
      "    {%- for tool in tools %}\n",
      "    {{- tool | tojson(indent=4) }}\n",
      "    {%- if not loop.last %}\n",
      "        {{- '\n",
      "\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- endfor %}\n",
      "    {{- '<|end_of_text|>\n",
      "' }}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "    {{- '<|start_of_role|>system<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'user' %}\n",
      "    {{- '<|start_of_role|>user<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>'  + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'assistant_tool_call' %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|><|tool_call|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- elif message['role'] == 'tool_response' %}\n",
      "    {{- '<|start_of_role|>tool_response<|end_of_role|>' + message['content'] + '<|end_of_text|>\n",
      "' }}\n",
      "    {%- endif %}\n",
      "    {%- if loop.last and add_generation_prompt %}\n",
      "    {{- '<|start_of_role|>assistant<|end_of_role|>' }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "Using chat eos_token: <|end_of_text|>\n",
      "Using chat bos_token: <|end_of_text|>\n",
      "llama_perf_context_print:        load time =  117045.50 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /   468 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /    39 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =  124885.12 ms /   507 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The current weather in Glasgow, UK is described as 'few clouds' with a temperature of 8.43 degrees Celsius and humidity at 76%.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation2 = conversation + [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Display the tool response in natural language.\" },\n",
    "    {\"role\": \"tool_response\", \"content\": str(tool_response) },\n",
    "]\n",
    "\n",
    "instruction_2 = TOKENIZER.apply_chat_template(conversation=conversation2, tools=tools, tokenize=False, add_generation_prompt=True)\n",
    "data_2 = make_api_request(instruction_2)\n",
    "data_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ffa708-1001-4efb-bb2a-716b933e91f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
